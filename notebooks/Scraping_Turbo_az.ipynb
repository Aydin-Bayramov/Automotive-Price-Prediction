{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports and Logging Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='../logs/scraping.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WebDriver Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.set_page_load_timeout(120)\n",
    "driver.set_script_timeout(120)\n",
    "wait = WebDriverWait(driver, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Scraping Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = {\n",
    "    \"Mercedes\": \"https://turbo.az/autos?page={}&q%5Bmake%5D%5B%5D=4&q%5Byear_from%5D=2019\",\n",
    "    \"Hyundai\": \"https://turbo.az/autos?page={}&q%5Bmake%5D%5B%5D=1&q%5Byear_from%5D=2019\",\n",
    "    \"Kia\": \"https://turbo.az/autos?page={}&q%5Bmake%5D%5B%5D=8&q%5Byear_from%5D=2019\",\n",
    "    \"BMW\": \"https://turbo.az/autos?page={}&q%5Bmake%5D%5B%5D=3&q%5Byear_from%5D=2019\"\n",
    "}\n",
    "\n",
    "all_data = {}\n",
    "\n",
    "for brand, url in brands.items():\n",
    "    print(f\"Scraping data for {brand}...\")\n",
    "    data = {\n",
    "        \"Price\": [], \"Make\": [], \"Model\": [], \"Year\": [],\n",
    "        \"Color\": [], \"Engine\": [], \"Kilometer\": [], \"Transmission\": [], \"New\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        for page in range(1, 23):  # 22 pages max\n",
    "            # Retry mechanism for page loading\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    driver.get(url.format(page))\n",
    "                    time.sleep(5)\n",
    "                    break\n",
    "                except Exception as error:\n",
    "                    logging.error(f\"Failed to load page {page}. Retrying ({attempt + 1}/{max_retries})... Error: {error}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise error\n",
    "\n",
    "            # Retry mechanism for fetching listings\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    listings = wait.until(EC.visibility_of_all_elements_located((By.CLASS_NAME, \"products-i__link\")))\n",
    "                    break\n",
    "                except Exception as error:\n",
    "                    logging.error(f\"Failed to fetch listings on page {page}. Retrying ({attempt + 1}/{max_retries})... Error: {error}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise error\n",
    "\n",
    "            # Scrape data for each listing\n",
    "            for listing in listings:\n",
    "                try:\n",
    "                    wait.until(EC.element_to_be_clickable(listing))\n",
    "                    listing.click()\n",
    "                    driver.switch_to.window(driver.window_handles[-1])  # Switch to the new tab\n",
    "\n",
    "                    # Retry mechanism for extracting data\n",
    "                    for attempt in range(max_retries):\n",
    "                        try:\n",
    "                            data[\"Price\"].append(wait.until(EC.presence_of_element_located((By.XPATH, \"//div[@class='product-price__i product-price__i--bold']\"))).text.strip())\n",
    "                            data[\"Make\"].append(wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"label[for='ad_make_id'] + span\"))).text.strip())\n",
    "                            data[\"Model\"].append(wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"label[for='ad_model'] + span\"))).text.strip())\n",
    "                            data[\"Year\"].append(wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"label[for='ad_reg_year'] + span\"))).text.strip())\n",
    "                            data[\"Color\"].append(wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"label[for='ad_color'] + span\"))).text.strip())\n",
    "                            data[\"Engine\"].append(wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"label[for='ad_engine_volume'] + span\"))).text.strip())\n",
    "                            data[\"Kilometer\"].append(wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"label[for='ad_mileage'] + span\"))).text.strip())\n",
    "                            data[\"Transmission\"].append(wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"label[for='ad_transmission'] + span\"))).text.strip())\n",
    "                            data[\"New\"].append(wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"label[for='ad_new'] + span\"))).text.strip())\n",
    "                            break\n",
    "                        except Exception as error:\n",
    "                            logging.error(f\"Failed to extract data. Retrying ({attempt + 1}/{max_retries})... Error: {error}\")\n",
    "                            if attempt == max_retries - 1:\n",
    "                                raise error\n",
    "\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])  # Switch back to the main tab\n",
    "                except Exception as error:\n",
    "                    logging.error(f\"Error extracting data from a listing: {error}\")\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    except Exception as error:\n",
    "        logging.critical(f\"Critical error occurred while scraping {brand}: {error}\")\n",
    "    finally:\n",
    "        all_data[brand] = pd.DataFrame(data)\n",
    "        print(f\"Data for {brand} scraped successfully.\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saving Scraped Data to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for brand, df in all_data.items():\n",
    "    df.to_csv(f\"../data/raw/{brand.lower()}.csv\", index=False)\n",
    "    print(f\"Data for {brand} saved to data/raw/{brand.lower()}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
